{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOYLJgF9bgkOgHkJ83C0pLF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Experiments on vanilla NADE(Neural Autoregressive Density Estimation)\n","* [paper](http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf)\n","* GPU is enabled: \n","> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**\n","\n","\n","<table align=\"left\"><td>\n","<a target=\"_blank\" href=\"https://colab.research.google.com/drive/1tzlvd-fzBgRP0-rhSePW9Fyq6B_RSnN3?usp=sharing\">\n","    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n","</td><td>\n","<a target=\"_blank\" href=\"\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"],"metadata":{"id":"hjDWMH9VJHrz"}},{"cell_type":"markdown","source":["# 01. Set-up"],"metadata":{"id":"GqI3epvPY8OZ"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Kr5FZ8IeVzig","colab":{"base_uri":"https://localhost:8080/","height":131},"executionInfo":{"status":"error","timestamp":1666131553925,"user_tz":300,"elapsed":11,"user":{"displayName":"Sowon Jeong","userId":"03227232882159269087"}},"outputId":"94755519-41b7-457d-e30d-6a86910cbe46"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-03f774f3e4a3>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    drive.mount(‘/content/gdrive’)\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"]}],"source":["from google.colab import drive\n","drive.mount(‘/content/gdrive’)"]},{"cell_type":"code","source":["! ls"],"metadata":{"id":"GopCd-bMLhNk","executionInfo":{"status":"ok","timestamp":1666131565291,"user_tz":300,"elapsed":506,"user":{"displayName":"Sowon Jeong","userId":"03227232882159269087"}},"outputId":"c2ee324a-2d73-4c10-86b0-2adf38ec95d8","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data\n"]}]},{"cell_type":"markdown","source":["# 02. NADE architecture"],"metadata":{"id":"zSpL_IeaZAcY"}},{"cell_type":"code","source":["# Copyright 2011 Hugo Larochelle. All rights reserved.\n","# \n","# Redistribution and use in source and binary forms, with or without modification, are\n","# permitted provided that the following conditions are met:\n","# \n","#    1. Redistributions of source code must retain the above copyright notice, this list of\n","#       conditions and the following disclaimer.\n","# \n","#    2. Redistributions in binary form must reproduce the above copyright notice, this list\n","#       of conditions and the following disclaimer in the documentation and/or other materials\n","#       provided with the distribution.\n","# \n","# THIS SOFTWARE IS PROVIDED BY Hugo Larochelle ``AS IS'' AND ANY EXPRESS OR IMPLIED\n","# WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\n","# FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL Hugo Larochelle OR\n","# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n","# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n","# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n","# ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n","# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n","# ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","# \n","# The views and conclusions contained in the software and documentation are those of the\n","# authors and should not be interpreted as representing official policies, either expressed\n","# or implied, of Hugo Larochelle.\n","\n","import mlpython.learners.generic as mlgeneric\n","import mlpython.mathutils.nonlinear as mlnonlin\n","import mlpython.mathutils.linalg as mllin\n","import numpy as np\n","\n","class NADE(mlgeneric.OnlineLearner):\n","   \"\"\"\n","   Neural Autoregressive Distribution Estimator (NADE) for multivariate binary distribution estimation\n","\n","   Options:\n","   - 'n_stages':           number of training iterations\n","   - 'learning_rate':      learning rate\n","   - 'decrease_constant':  decrease constant\n","   - 'untied_weights':     whether to untie the weights going into and out of the hidden units\n","   - 'hidden_size':        number of hidden units\n","   - 'input_order':        list of integers corresponding to the order for input modeling\n","   - 'seed':               seed for randomly initializing the weights\n","\n","   Required metadata:\n","   - 'input_size'\n","\n","   \"\"\"\n","\n","   def initialize_learner(self,metadata):\n","      self.rng = np.random.mtrand.RandomState(self.seed)\n","      self.input_size = metadata['input_size']\n","      if self.hidden_size <= 0:\n","          raise ValueError('hidden_size should be > 0')\n","\n","      self.W = (2*self.rng.rand(self.hidden_size,self.input_size)-1)/self.input_size\n","      self.c = np.zeros((self.hidden_size))\n","      self.b = np.zeros((self.input_size))\n","\n","      self.dW = np.zeros((self.hidden_size,self.input_size))\n","      self.dc = np.zeros((self.hidden_size))\n","      self.db = np.zeros((self.input_size))\n","\n","      if self.untied_weights:\n","          self.V = (2*self.rng.rand(self.hidden_size,self.input_size)-1)/self.input_size\n","          self.dV = np.zeros((self.hidden_size,self.input_size))\n","\n","      self.input = np.zeros((self.input_size))\n","      self.input_times_W = np.zeros((self.hidden_size,self.input_size))\n","      self.acc_input_times_W = np.zeros((self.hidden_size,self.input_size))\n","      self.hid = np.zeros((self.hidden_size,self.input_size))\n","      self.Whid = np.zeros((self.hidden_size,self.input_size))\n","      self.recact = np.zeros((self.input_size))\n","      self.rec = np.zeros((self.input_size))\n","      \n","      self.dinput_times_W = np.zeros((self.hidden_size,self.input_size))\n","      self.dacc_input_times_W = np.zeros((self.hidden_size,self.input_size))\n","      self.dhid = np.zeros((self.hidden_size,self.input_size))\n","      self.dWhid = np.zeros((self.hidden_size,self.input_size))\n","      self.dWenc = np.zeros((self.hidden_size,self.input_size))\n","      self.drecact = np.zeros((self.input_size))\n","      self.drec = np.zeros((self.input_size))\n","      \n","      self.n_updates = 0\n","\n","   def update_learner(self,example):\n","      self.input[self.input_order] = example\n","   \n","      # fprop\n","      np.multiply(self.input,self.W,self.input_times_W)\n","      np.add.accumulate(self.input_times_W[:,:-1],axis=1,out=self.acc_input_times_W[:,1:])\n","      self.acc_input_times_W[:,0] = 0\n","      self.acc_input_times_W += self.c[:,np.newaxis]\n","      mlnonlin.sigmoid(self.acc_input_times_W,self.hid)\n","\n","      if self.untied_weights:\n","          np.multiply(self.hid,self.V,self.Whid)\n","      else:\n","          np.multiply(self.hid,self.W,self.Whid)\n","\n","      mllin.sum_columns(self.Whid,self.recact)\n","      self.recact += self.b\n","      mlnonlin.sigmoid(self.recact,self.rec)\n","\n","      # bprop\n","      np.subtract(self.rec,self.input,self.drec)\n","      self.db[:] = self.drec\n","\n","      if self.untied_weights:\n","          np.multiply(self.drec,self.hid,self.dV)\n","          np.multiply(self.drec,self.V,self.dhid)\n","          self.dW[:] = 0\n","      else:\n","          np.multiply(self.drec,self.hid,self.dW)\n","          np.multiply(self.drec,self.W,self.dhid)\n","\n","      mlnonlin.dsigmoid(self.hid,self.dhid,self.dacc_input_times_W)\n","      mllin.sum_rows(self.dacc_input_times_W,self.dc)      \n","      np.add.accumulate(self.dacc_input_times_W[:,:0:-1],axis=1,out=self.dWenc[:,-2::-1])\n","      self.dWenc[:,-1] = 0\n","      self.dWenc *= self.input\n","      self.dW += self.dWenc\n","\n","      self.dW *= self.learning_rate/(1.+self.decrease_constant*self.n_updates)\n","      self.db *= self.learning_rate/(1.+self.decrease_constant*self.n_updates)\n","      self.dc *= self.learning_rate/(1.+self.decrease_constant*self.n_updates)\n","\n","      self.W -= self.dW\n","      self.b -= self.db\n","      self.c -= self.dc\n","\n","      if self.untied_weights:\n","          self.dV *= self.learning_rate/(1.+self.decrease_constant*self.n_updates)\n","          self.V -= self.dV\n","      self.n_updates += 1\n","\n","   def use_learner(self,example):\n","      self.input[self.input_order] = example\n","      output = np.zeros((self.input_size))\n","      recact = np.zeros((self.input_size))\n","   \n","      # fprop\n","      np.multiply(self.input,self.W,self.input_times_W)\n","      np.add.accumulate(self.input_times_W[:,:-1],axis=1,out=self.acc_input_times_W[:,1:])\n","      self.acc_input_times_W[:,0] = 0\n","      self.acc_input_times_W += self.c[:,np.newaxis]\n","      mlnonlin.sigmoid(self.acc_input_times_W,self.hid)\n","      if self.untied_weights:\n","          np.multiply(self.hid,self.V,self.Whid)\n","      else:\n","          np.multiply(self.hid,self.W,self.Whid)\n","\n","      mllin.sum_columns(self.Whid,recact)\n","      recact += self.b\n","      mlnonlin.sigmoid(recact,output)\n","      return [output,recact]\n","\n","   def cost(self,outputs,example):\n","      self.input[self.input_order] = example\n","      #return [ np.sum(-self.input*np.log(outputs[0]) - (1-self.input)*np.log(1-outputs[0])) ]\n","      return [ np.sum(-self.input*(outputs[1]-np.log(1+np.exp(outputs[1]))) - (1-self.input)*(-outputs[1]-np.log(1+np.exp(-outputs[1])))) ]\n","\n","   def sample(self):\n","      input = np.zeros(self.input_size)\n","      input_prob = np.zeros(self.input_size)\n","      hid_i = np.zeros(self.hidden_size)\n","      for i in range(self.input_size):\n","         if i > 0:\n","            mlnonlin.sigmoid(self.c+np.dot(self.W[:,:i],input[:i]),hid_i)\n","         else:\n","            mlnonlin.sigmoid(self.c,hid_i)\n","\n","         if self.untied_weights:\n","            mlnonlin.sigmoid(np.dot(hid_i,self.V[:,i])+self.b[i:i+1],input_prob[i:i+1])\n","         else:\n","            mlnonlin.sigmoid(np.dot(hid_i,self.W[:,i])+self.b[i:i+1],input_prob[i:i+1])\n","\n","         input[i] = (self.rng.rand()<input_prob[i])\n","\n","      return (input[self.input_order],input_prob[self.input_order])\n","\n","   def verify_gradients(self,untied_weights):\n","      \n","      print('WARNING: calling verify_gradients reinitializes the learner')\n","\n","      rng = np.random.mtrand.RandomState(1234)\n","      input_order = range(20)\n","      rng.shuffle(input_order)\n","\n","      self.seed = 1234\n","      self.hidden_size = 10\n","      self.input_order = input_order\n","      self.untied_weights = untied_weights\n","      self.initialize_learner({'input_size':20})\n","      example = rng.rand(20)<0.5\n","      epsilon=1e-6\n","      self.learning_rate = 1\n","      self.decrease_constant = 0\n","\n","      W_copy = np.array(self.W)\n","      emp_dW = np.zeros(self.W.shape)\n","      for i in range(self.W.shape[0]):\n","         for j in range(self.W.shape[1]):\n","            self.W[i,j] += epsilon\n","            output = self.use_learner(example)\n","            a = self.cost(output,example)[0]\n","            self.W[i,j] -= epsilon\n","\n","            self.W[i,j] -= epsilon\n","            output = self.use_learner(example)\n","            b = self.cost(output,example)[0]\n","            self.W[i,j] += epsilon\n","\n","            emp_dW[i,j] = (a-b)/(2.*epsilon)\n","\n","      self.update_learner(example)\n","      self.W[:] = W_copy\n","      print('dW diff.:',np.sum(np.abs(self.dW.ravel()-emp_dW.ravel()))/self.W.ravel().shape[0])\n","\n","      b_copy = np.array(self.b)\n","      emp_db = np.zeros(self.b.shape)\n","      for i in range(self.b.shape[0]):\n","         self.b[i] += epsilon\n","         output = self.use_learner(example)\n","         a = self.cost(output,example)[0]\n","         self.b[i] -= epsilon\n","         \n","         self.b[i] -= epsilon\n","         output = self.use_learner(example)\n","         b = self.cost(output,example)[0]\n","         self.b[i] += epsilon\n","         \n","         emp_db[i] = (a-b)/(2.*epsilon)\n","\n","      self.update_learner(example)\n","      self.b[:] = b_copy\n","      print('db diff.:',np.sum(np.abs(self.db.ravel()-emp_db.ravel()))/self.b.ravel().shape[0])\n","\n","      c_copy = np.array(self.c)\n","      emp_dc = np.zeros(self.c.shape)\n","      for i in range(self.c.shape[0]):\n","         self.c[i] += epsilon\n","         output = self.use_learner(example)\n","         a = self.cost(output,example)[0]\n","         self.c[i] -= epsilon\n","\n","         self.c[i] -= epsilon\n","         output = self.use_learner(example)\n","         b = self.cost(output,example)[0]\n","         self.c[i] += epsilon\n","\n","         emp_dc[i] = (a-b)/(2.*epsilon)\n","\n","      self.update_learner(example)\n","      self.c[:] = c_copy\n","      print('dc diff.:',np.sum(np.abs(self.dc.ravel()-emp_dc.ravel()))/self.c.ravel().shape[0])\n","\n","      if untied_weights:\n","         V_copy = np.array(self.V)\n","         emp_dV = np.zeros(self.V.shape)\n","         for i in range(self.V.shape[0]):\n","            for j in range(self.V.shape[1]):\n","               self.V[i,j] += epsilon\n","               output = self.use_learner(example)\n","               a = self.cost(output,example)[0]\n","               self.V[i,j] -= epsilon\n","         \n","               self.V[i,j] -= epsilon\n","               output = self.use_learner(example)\n","               b = self.cost(output,example)[0]\n","               self.V[i,j] += epsilon\n","         \n","               emp_dV[i,j] = (a-b)/(2.*epsilon)\n","         \n","         self.update_learner(example)\n","         self.V[:] = V_copy\n","         print('dV diff.:',np.sum(np.abs(self.dV.ravel()-emp_dV.ravel()))/self.V.ravel().shape[0])\n","\n","\n"],"metadata":{"id":"tfYWQkmFZmsr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 03. Training"],"metadata":{"id":"A7RkbyFTZC_M"}},{"cell_type":"markdown","source":["# 04. Extension: What if using ReLU instead of Sigmoid?"],"metadata":{"id":"hvJ4H1qKZE5O"}}]}